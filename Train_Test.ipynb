{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas numpy joblib shap matplotlib seaborn scikit-learn xgboost lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgKt2uFUEish",
        "outputId": "f0b8247e-8261-4158-cd84-f4f9007a8534"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os\n",
        "\n",
        "# ========== Load and Clean Function ==========\n",
        "def load_and_prepare(file):\n",
        "    df = pd.read_csv(file)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    label_col = next((col for col in df.columns if 'label' in col or 'class' in col), None)\n",
        "    if label_col is None:\n",
        "        raise ValueError(f\"No label column found in {file}\")\n",
        "    df.rename(columns={label_col: 'label'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "# ========== Load and Combine Data ==========\n",
        "csv_files = [\n",
        "    \"data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
        "    \"data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "    \"data/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "    \"data/Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "    \"data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "    \"data/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"data/Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìÇ Reading and cleaning CSV files...\")\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = load_and_prepare(file)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Skipping {file}: {e}\")\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Labels before encoding:\", df_all['label'].unique())\n",
        "\n",
        "# ========== Label Encoding ==========\n",
        "label_encoder = LabelEncoder()\n",
        "df_all['label'] = label_encoder.fit_transform(df_all['label'])\n",
        "print(\"Encoded classes:\", label_encoder.classes_)\n",
        "\n",
        "# ========== Feature Prep ==========\n",
        "X = df_all.drop(columns=['label'])\n",
        "y = df_all['label']\n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# ========== Train-Test Split ==========\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Feature Scaling ==========\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(scaler, \"models/scaler.pkl\")\n",
        "\n",
        "# ========== Feature Selection ==========\n",
        "print(\"\\nüéØ Performing Feature Selection...\")\n",
        "selector = SelectKBest(f_classif, k=20)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X.columns[selected_indices]\n",
        "joblib.dump(selector, \"selector.pkl\")\n",
        "\n",
        "# ========== Save test data and selector ==========\n",
        "joblib.dump(X_test, \"X_test.pkl\")\n",
        "joblib.dump(y_test, \"y_test.pkl\")\n",
        "\n",
        "# ========== Random Forest ==========\n",
        "print(\"\\nüå≤ Training Random Forest...\")\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None]\n",
        "}, cv=3, n_jobs=-1, verbose=1)\n",
        "rf_grid.fit(X_train_selected, y_train)\n",
        "rf_best = rf_grid.best_estimator_\n",
        "rf_preds = rf_best.predict(X_test_selected)\n",
        "print(f\"‚úÖ RF Accuracy: {accuracy_score(y_test, rf_preds):.4f}\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "joblib.dump(rf_best, \"models/random_forest.pkl\")\n",
        "\n",
        "# ========== XGBoost ==========\n",
        "print(\"\\nüì¶ Training XGBoost...\")\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
        "    {\n",
        "        'n_estimators': [100, 150],\n",
        "        'max_depth': [4, 6],\n",
        "        'learning_rate': [0.1, 0.2]\n",
        "    },\n",
        "    n_iter=3,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_random.fit(X_train_selected, y_train)\n",
        "xgb_best = xgb_random.best_estimator_\n",
        "xgb_preds = xgb_best.predict(X_test_selected)\n",
        "print(f\"‚úÖ XGB Accuracy: {accuracy_score(y_test, xgb_preds):.4f}\")\n",
        "print(classification_report(y_test, xgb_preds))\n",
        "joblib.dump(xgb_best, \"models/xgboost.pkl\")\n",
        "\n",
        "# ========== SHAP Explainability ==========\n",
        "print(\"\\nüß† Generating SHAP summary plot...\")\n",
        "explainer = shap.Explainer(xgb_best)\n",
        "shap_values = explainer(X_test_selected)\n",
        "shap.summary_plot(shap_values, pd.DataFrame(X_test_selected, columns=selected_feature_names), show=False)\n",
        "plt.savefig(\"shap_summary.png\")\n",
        "plt.close()\n",
        "print(\"SHAP summary plot saved as shap_summary.png\")\n",
        "\n",
        "# ========== Deep Neural Network ==========\n",
        "print(\"\\nüß† Training Deep Neural Network...\")\n",
        "num_classes = len(np.unique(y_train))\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(X_train_selected.shape[1],), activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add a callback to show progress after each epoch\n",
        "class PrintEpochProgress(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch+1}: loss={logs['loss']:.4f}, acc={logs['accuracy']:.4f}, val_loss={logs.get('val_loss', 0):.4f}, val_acc={logs.get('val_accuracy', 0):.4f}\")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_selected, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    verbose=1,  # Shows progress bar\n",
        "    validation_split=0.1,  # Optional: shows validation metrics\n",
        "    callbacks=[PrintEpochProgress(), early_stop]\n",
        ")\n",
        "dnn_preds = np.argmax(model.predict(X_test_selected), axis=1)\n",
        "print(f\"‚úÖ DNN Accuracy: {accuracy_score(y_test, dnn_preds):.4f}\")\n",
        "print(classification_report(y_test, dnn_preds))\n",
        "model.save(\"models/dnn_model.keras\")\n",
        "\n",
        "# ========== Accuracy Summary ==========\n",
        "print(\"\\nüìä Final Accuracy Comparison:\")\n",
        "print(f\"Random Forest : {accuracy_score(y_test, rf_preds):.4f}\")\n",
        "print(f\"XGBoost       : {accuracy_score(y_test, xgb_preds):.4f}\")\n",
        "print(f\"DNN           : {accuracy_score(y_test, dnn_preds):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ========== Load and Clean Function ==========\n",
        "def load_and_prepare(file):\n",
        "    df = pd.read_csv(file)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    label_col = next((col for col in df.columns if 'label' in col or 'class' in col), None)\n",
        "    if label_col is None:\n",
        "        raise ValueError(f\"No label column found in {file}\")\n",
        "    df.rename(columns={label_col: 'label'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "# ========== Load and Combine Data ==========\n",
        "csv_files = [\n",
        "    \"data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
        "    \"data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "    \"data/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "    \"data/Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "    \"data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "    \"data/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "    \"data/Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìÇ Reading and cleaning CSV files...\")\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = load_and_prepare(file)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Skipping {file}: {e}\")\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Labels before encoding:\", df_all['label'].unique())\n",
        "\n",
        "# ========== Label Encoding ==========\n",
        "label_encoder = LabelEncoder()\n",
        "df_all['label'] = label_encoder.fit_transform(df_all['label'])\n",
        "print(\"Encoded classes:\", label_encoder.classes_)\n",
        "\n",
        "# ========== Feature Prep ==========\n",
        "X = df_all.drop(columns=['label'])\n",
        "y = df_all['label']\n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# ========== Train-Test Split ==========\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Feature Scaling ==========\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(scaler, \"models/scaler.pkl\")\n",
        "\n",
        "# ========== Feature Selection ==========\n",
        "print(\"\\nüéØ Performing Feature Selection...\")\n",
        "selector = SelectKBest(f_classif, k=20)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X.columns[selected_indices]\n",
        "joblib.dump(selector, \"models/selector.pkl\")\n",
        "\n",
        "# ========== Save test data and selector ==========\n",
        "joblib.dump(X_test, \"models/X_test.pkl\")\n",
        "joblib.dump(y_test, \"models/y_test.pkl\")\n",
        "\n",
        "# ========== XGBoost ==========\n",
        "print(\"\\nüì¶ Training XGBoost with early stopping and progress...\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "eval_set = [(X_train_selected, y_train), (X_test_selected, y_test)]\n",
        "xgb_model.fit(\n",
        "    X_train_selected, y_train,\n",
        "    eval_set=eval_set,\n",
        "    early_stopping_rounds=10,\n",
        "    verbose=True\n",
        ")\n",
        "xgb_preds = xgb_model.predict(X_test_selected)\n",
        "print(f\"‚úÖ XGB Accuracy: {accuracy_score(y_test, xgb_preds):.4f}\")\n",
        "print(classification_report(y_test, xgb_preds))\n",
        "joblib.dump(xgb_model, \"models/xgboost.pkl\")\n",
        "\n",
        "# ========== SHAP Explainability ==========\n",
        "print(\"\\nüß† Generating SHAP summary plot...\")\n",
        "explainer = shap.Explainer(xgb_model)\n",
        "shap_values = explainer(X_test_selected)\n",
        "shap.summary_plot(shap_values, pd.DataFrame(X_test_selected, columns=selected_feature_names), show=False)\n",
        "plt.savefig(\"shap_summary.png\")\n",
        "plt.close()\n",
        "print(\"SHAP summary plot saved as shap_summary.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load test data, selector, scaler, and models\n",
        "X_test = joblib.load(\"models/X_test.pkl\")\n",
        "y_test = joblib.load(\"models/y_test.pkl\")\n",
        "selector = joblib.load(\"models/selector.pkl\")\n",
        "scaler = joblib.load(\"models/scaler.pkl\")\n",
        "xgb_model = joblib.load(\"models/xgboost.pkl\")\n",
        "dnn_model = load_model(\"models/dnn_model.keras\")\n",
        "rf_model = joblib.load(\"models/random_forest.pkl\")\n",
        "X_train_selected = joblib.load(\"models/X_train_selected.pkl\")\n",
        "selected_feature_names = joblib.load(\"models/selected_feature_names.pkl\")\n",
        "y_train = joblib.load(\"models/y_train.pkl\")\n",
        "\n",
        "# Scale and select features\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_selected = X_test_scaled[:, selector.get_support(indices=True)]\n",
        "\n",
        "# Random Forest predictions\n",
        "rf_preds = rf_model.predict(X_test_selected)\n",
        "print(\"Random Forest Test Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_preds):.4f}\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "\n",
        "# XGBoost predictions\n",
        "xgb_preds = xgb_model.predict(X_test_selected)\n",
        "print(\"XGBoost Test Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, xgb_preds):.4f}\")\n",
        "print(classification_report(y_test, xgb_preds))\n",
        "\n",
        "\n",
        "# DNN predictions\n",
        "dnn_probs = dnn_model.predict(X_test_selected)\n",
        "dnn_preds = np.argmax(dnn_probs, axis=1)\n",
        "print(\"DNN Test Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dnn_preds):.4f}\")\n",
        "print(classification_report(y_test, dnn_preds))\n",
        "\n",
        "\n",
        "# Prepare LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train_selected),\n",
        "    feature_names=list(selected_feature_names),\n",
        "    class_names=[str(cls) for cls in np.unique(y_train)],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# Pick a test sample to explain (e.g., the first one)\n",
        "for i in range(5):  # Explain first 5 samples\n",
        "    exp = explainer.explain_instance(\n",
        "        X_test_selected[i],\n",
        "        xgb_model.predict_proba,\n",
        "        num_features=10\n",
        "    )\n",
        "    exp.save_to_file(f'html/lime_xgboost_explanation_{i}.html')\n",
        "    print(f\"LIME explanation for sample {i} saved as lime_xgboost_explanation_{i}.html\")\n",
        "    print(exp.as_list())\n",
        "\n",
        "exp.save_to_file('html/lime_xgboost_explanation.html')\n",
        "print(\"LIME explanation saved as lime_xgboost_explanation.html\")\n",
        "\n",
        "\n",
        "# SHAP summary plot for XGBoost\n",
        "explainer = shap.Explainer(xgb_model)\n",
        "shap_values = explainer(X_test_selected)\n",
        "shap.summary_plot(shap_values, pd.DataFrame(X_test_selected, columns=X_test.columns[selector.get_support()]), show=False)\n",
        "plt.savefig(\"Output/shap_summary.png\")\n",
        "plt.close()\n",
        "print(\"SHAP summary plot saved as shap_summary.png\")\n",
        "\n",
        "\n",
        "# Plot confusion matrix for Random Forest\n",
        "cm = confusion_matrix(y_test, xgb_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', cbar=True,xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('XGBoost Confusion Matrix (Seaborn Heatmap)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Output/xgboost_confusion_matrix.png\")\n",
        "plt.close()\n",
        "print(\"XGBoost confusion matrix plot saved as xgboost_confusion_matrix.png\")\n",
        "\n",
        "# Plot classification report as a heatmap\n",
        "report = classification_report(y_test, xgb_preds, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"XGBoost Classification Report Heatmap\")\n",
        "plt.savefig(\"Output/xgboost_classification_report.png\")\n",
        "plt.close()\n",
        "print(\"XGBoost classification report heatmap saved as xgboost_classification_report.png\")\n",
        "\n",
        "\n",
        "# Save all accuracies to results.tex\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "dnn_accuracy = accuracy_score(y_test, dnn_preds)\n",
        "\n",
        "with open(\"Result/results.tex\", \"w\") as f:\n",
        "    f.write(\"\\\\section*{Model Accuracy Results}\\n\")\n",
        "    f.write(f\"\\\\textbf{{Random Forest Accuracy}}: {rf_accuracy:.4f}\\\\\\\\\\n\")\n",
        "    f.write(f\"\\\\textbf{{XGBoost Accuracy}}: {xgb_accuracy:.4f}\\\\\\\\\\n\")\n",
        "    f.write(f\"\\\\textbf{{DNN Accuracy}}: {dnn_accuracy:.4f}\\\\\\\\\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
